{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using CNN for text classification\n",
    "\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test data\n",
    "train  = pd.read_csv('SemEval2018-T3-train-taskA_emoji.txt', sep=\"\\t\")\n",
    "test = pd.read_csv('SemEval2018-T3_gold_test_taskA_emoji.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine training and test\n",
    "combi = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n",
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['Tweet text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>We are rumored to have talked to Erv's agent....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>If you drag yesterday into today, your tomorro...</td>\n",
       "      <td>If you drag yesterday into today, your tomorro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>781</td>\n",
       "      <td>0</td>\n",
       "      <td>Congrats to my fav @JennyBrew &amp; her team &amp; my ...</td>\n",
       "      <td>Congrats to my fav  &amp; her team &amp; my birthplace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>782</td>\n",
       "      <td>0</td>\n",
       "      <td>@allkpop: Jessica sheds tears at her fan signi...</td>\n",
       "      <td>: Jessica sheds tears at her fan signing event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>783</td>\n",
       "      <td>1</td>\n",
       "      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n",
       "      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>#NOT ALL ðŸ‘Œ There good &amp; bad in every occupatio...</td>\n",
       "      <td>#NOT ALL ðŸ‘Œ There good &amp; bad in every occupatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet index  Label                                         Tweet text  \\\n",
       "0               1      1  Sweet United Nations video. Just in time for C...   \n",
       "1               2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2               3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3               4      0                3 episodes left I'm dying over here   \n",
       "4               5      1  I can't breathe! was chosen as the most notabl...   \n",
       "...           ...    ...                                                ...   \n",
       "4596          780      0  If you drag yesterday into today, your tomorro...   \n",
       "4597          781      0  Congrats to my fav @JennyBrew & her team & my ...   \n",
       "4598          782      0  @allkpop: Jessica sheds tears at her fan signi...   \n",
       "4599          783      1  #Irony: al jazeera is pro Anti - #GamerGate be...   \n",
       "4600          784      0  #NOT ALL ðŸ‘Œ There good & bad in every occupatio...   \n",
       "\n",
       "                                             tidy_tweet  \n",
       "0     Sweet United Nations video. Just in time for C...  \n",
       "1      We are rumored to have talked to Erv's agent....  \n",
       "2     Hey there! Nice to see you Minnesota/ND Winter...  \n",
       "3                   3 episodes left I'm dying over here  \n",
       "4     I can't breathe! was chosen as the most notabl...  \n",
       "...                                                 ...  \n",
       "4596  If you drag yesterday into today, your tomorro...  \n",
       "4597  Congrats to my fav  & her team & my birthplace...  \n",
       "4598  : Jessica sheds tears at her fan signing event...  \n",
       "4599  #Irony: al jazeera is pro Anti - #GamerGate be...  \n",
       "4600  #NOT ALL ðŸ‘Œ There good & bad in every occupatio...  \n",
       "\n",
       "[4601 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words less than 3 characters\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, stemming and lemmentization\n",
    "def utils_preprocess_text(text, flg_stemm=True, flg_lemm =True, lst_stopwords=None ):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    #tokenization(convert from string to List)\n",
    "    lst_text = text.split()\n",
    "    #remove stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in\n",
    "                   lst_stopwords]\n",
    "        \n",
    "     #stemming\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "        \n",
    "    #Lemmentization\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "        \n",
    "    # back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Cheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: utils_preprocess_text(x, flg_stemm = False, flg_lemm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "      <td>sweet united nation video just time christmas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "      <td>rumored have talked agent angel asked about es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "      <td>there nice minnesota winter weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "      <td>episode left dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "      <td>breathe chosen most notable quote year annual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>If you drag yesterday into today, your tomorro...</td>\n",
       "      <td>drag yesterday into today your tomorrow cannot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>781</td>\n",
       "      <td>0</td>\n",
       "      <td>Congrats to my fav @JennyBrew &amp; her team &amp; my ...</td>\n",
       "      <td>congrats team birthplace team ohiostate claim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>782</td>\n",
       "      <td>0</td>\n",
       "      <td>@allkpop: Jessica sheds tears at her fan signi...</td>\n",
       "      <td>jessica shed tear signing event http http uegd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>783</td>\n",
       "      <td>1</td>\n",
       "      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n",
       "      <td>irony jazeera anti gamergate because feminism ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>784</td>\n",
       "      <td>0</td>\n",
       "      <td>#NOT ALL ðŸ‘Œ There good &amp; bad in every occupatio...</td>\n",
       "      <td>not there good every occupation race stop labe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tweet index  Label                                         Tweet text  \\\n",
       "0               1      1  Sweet United Nations video. Just in time for C...   \n",
       "1               2      1  @mrdahl87 We are rumored to have talked to Erv...   \n",
       "2               3      1  Hey there! Nice to see you Minnesota/ND Winter...   \n",
       "3               4      0                3 episodes left I'm dying over here   \n",
       "4               5      1  I can't breathe! was chosen as the most notabl...   \n",
       "...           ...    ...                                                ...   \n",
       "4596          780      0  If you drag yesterday into today, your tomorro...   \n",
       "4597          781      0  Congrats to my fav @JennyBrew & her team & my ...   \n",
       "4598          782      0  @allkpop: Jessica sheds tears at her fan signi...   \n",
       "4599          783      1  #Irony: al jazeera is pro Anti - #GamerGate be...   \n",
       "4600          784      0  #NOT ALL ðŸ‘Œ There good & bad in every occupatio...   \n",
       "\n",
       "                                             tidy_tweet  \n",
       "0     sweet united nation video just time christmas ...  \n",
       "1     rumored have talked agent angel asked about es...  \n",
       "2                   there nice minnesota winter weather  \n",
       "3                          episode left dying over here  \n",
       "4     breathe chosen most notable quote year annual ...  \n",
       "...                                                 ...  \n",
       "4596  drag yesterday into today your tomorrow cannot...  \n",
       "4597  congrats team birthplace team ohiostate claim ...  \n",
       "4598  jessica shed tear signing event http http uegd...  \n",
       "4599  irony jazeera anti gamergate because feminism ...  \n",
       "4600  not there good every occupation race stop labe...  \n",
       "\n",
       "[4601 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000 # Maximum Number of words we want to include in our dictionary\n",
    "maxlen = 72 # No of words in question we want to create a sequence with\n",
    "embed_size = 300# Size of word to vec embedding we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training into train and val\n",
    "train_df, val_df = train_test_split(combi.iloc[:3816], test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(combi['tidy_tweet'])\n",
    "train_X = tokenizer.texts_to_sequences(train_df['tidy_tweet'])\n",
    "val_X = tokenizer.texts_to_sequences(val_df['tidy_tweet'])\n",
    "test_X = tokenizer.texts_to_sequences(combi.iloc[3817:]['tidy_tweet'])\n",
    "\n",
    "# padding\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1723\n",
      "1    1711\n",
      "Name: Label, dtype: int64\n",
      "0    192\n",
      "1    190\n",
      "Name: Label, dtype: int64\n",
      "0    473\n",
      "1    311\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_y = train_df['Label'].values\n",
    "val_y = val_df['Label'].values\n",
    "test_y = test['Label'].values\n",
    "\n",
    "print(train_df['Label'].value_counts())\n",
    "print(val_df['Label'].value_counts())\n",
    "print(test['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load the pre-trained embedding model glove. I did not have enough PC resource for that and ignored it\n",
    "\n",
    "# def get_coefs(word,*arr): \n",
    "#     return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# def get_embdedings_matrix(embeddings_index, word_index, nb_words = None):\n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     print('Shape of Full Embeddding Matrix', all_embs.shape)\n",
    "#     embed_dims = all_embs.shape[1]\n",
    "#     emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "#     #best to free up memory, given the size, which is usually ~3-4GB in memory\n",
    "#     del all_embs\n",
    "#     if nb_words is None:\n",
    "#         nb_words = len(word_index)\n",
    "#     else:\n",
    "#         nb_words = min(nb_words, len(word_index))\n",
    "    \n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_dims))\n",
    "#     found_vectors = 0\n",
    "#     words_not_found = []\n",
    "#     for word, i in tqdm(word_index.items()):\n",
    "#         if i >= nb_words: \n",
    "#             continue\n",
    "#         embedding_vector = None\n",
    "#         if word in embeddings_index:\n",
    "#             embedding_vector = embeddings_index.get(word)\n",
    "#         elif word.lower() in embeddings_index:\n",
    "#             embedding_vector = embeddings_index.get(word.lower())\n",
    "#         # for twitter check if the key is a hashtag\n",
    "#         elif '#'+word.lower() in embeddings_index:\n",
    "#             embedding_vector = embeddings_index.get('#'+word.lower())\n",
    "            \n",
    "#         if embedding_vector is not None: \n",
    "#             found_vectors += 1\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "#         else:\n",
    "#             words_not_found.append((word, i))\n",
    "\n",
    "#     print(\"% of Vectors found in Corpus\", found_vectors / nb_words)\n",
    "#     return embedding_matrix, words_not_found\n",
    "\n",
    "# # Word 2 vec Embedding\n",
    "# def load_glove(word_index):\n",
    "# #     print('Loading Glove')\n",
    "#     embed_file_path = 'glove840b300dtxt/glove.840B.300d.txt'\n",
    "#     embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_file_path, 'r', encoding='utf-8')))\n",
    "#     print(\"Built Embedding Index:\", len(embeddings_index))\n",
    "#     return get_embdedings_matrix(embeddings_index, word_index)\n",
    "\n",
    "# glove_embed_matrix, words_not_found =  load_glove(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn():\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, input_length=train_X.shape[1])(inp)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 72)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 72, 300)      30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 72, 300, 1)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 72, 1, 36)    10836       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 71, 1, 36)    21636       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 70, 1, 36)    32436       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 68, 1, 36)    54036       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 36)     0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4, 1, 36)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 144)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 144)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            145         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,119,089\n",
      "Trainable params: 30,119,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = model_cnn()\n",
    "# model = model_lstm_atten()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred(model, epochs=2):\n",
    "    filepath=\"weights_best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    for e in range(epochs):\n",
    "        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y),callbacks=callbacks)\n",
    "    model.load_weights(filepath)\n",
    "    pred_val_y = model.predict([val_X], batch_size=256, verbose=0)\n",
    "    pred_test_y = model.predict([test_X], batch_size=256, verbose=0)\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 12s 1s/step - loss: 0.6919 - accuracy: 0.5232 - val_loss: 0.6843 - val_accuracy: 0.5890\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68427, saving model to weights_best.h5\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.6448 - accuracy: 0.7845 - val_loss: 0.6737 - val_accuracy: 0.6047\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.68427 to 0.67372, saving model to weights_best.h5\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.5961 - accuracy: 0.8698 - val_loss: 0.6611 - val_accuracy: 0.6073\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.67372 to 0.66113, saving model to weights_best.h5\n",
      "7/7 [==============================] - 11s 2s/step - loss: 0.5365 - accuracy: 0.8797 - val_loss: 0.6480 - val_accuracy: 0.6073\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.66113 to 0.64804, saving model to weights_best.h5\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.4648 - accuracy: 0.8879 - val_loss: 0.6430 - val_accuracy: 0.6152\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.64804 to 0.64301, saving model to weights_best.h5\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.3768 - accuracy: 0.9202 - val_loss: 0.6480 - val_accuracy: 0.6047\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.64301\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.2851 - accuracy: 0.9485 - val_loss: 0.6710 - val_accuracy: 0.5942\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.64301\n",
      "7/7 [==============================] - 10s 1s/step - loss: 0.2020 - accuracy: 0.9656 - val_loss: 0.7148 - val_accuracy: 0.5812\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.64301\n"
     ]
    }
   ],
   "source": [
    "train_X = np.asarray(train_X)\n",
    "train_y = np.asarray(train_y)\n",
    "val_X = np.asarray(val_X)\n",
    "val_y = np.asarray(val_y)\n",
    "test_X = np.asarray(test_X)\n",
    "test_y = np.asarray(test_y)\n",
    "pred_val_y, pred_test_y = train_pred(model, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the best F1, we choose a suitable probability threshold\n",
    "def f1_smart(y_true, y_pred):\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        res = metrics.f1_score(y_true, (y_pred > thresh).astype(int))\n",
    "        thresholds.append([thresh, res])\n",
    "        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    best_f1 = thresholds[0][1]\n",
    "    print(\"Best threshold: \", best_thresh)\n",
    "    return  best_f1, best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.6643356643356644\n",
      "F1 score at threshold 0.11 is 0.6654991243432574\n",
      "F1 score at threshold 0.12 is 0.6654991243432574\n",
      "F1 score at threshold 0.13 is 0.6654991243432574\n",
      "F1 score at threshold 0.14 is 0.6654991243432574\n",
      "F1 score at threshold 0.15 is 0.6654991243432574\n",
      "F1 score at threshold 0.16 is 0.6654991243432574\n",
      "F1 score at threshold 0.17 is 0.6678383128295254\n",
      "F1 score at threshold 0.18 is 0.6690140845070423\n",
      "F1 score at threshold 0.19 is 0.6690140845070423\n",
      "F1 score at threshold 0.2 is 0.6690140845070423\n",
      "F1 score at threshold 0.21 is 0.6690265486725664\n",
      "F1 score at threshold 0.22 is 0.6702127659574467\n",
      "F1 score at threshold 0.23 is 0.6714031971580818\n",
      "F1 score at threshold 0.24 is 0.6737967914438504\n",
      "F1 score at threshold 0.25 is 0.6810810810810811\n",
      "F1 score at threshold 0.26 is 0.6823956442831216\n",
      "F1 score at threshold 0.27 is 0.676416819012797\n",
      "F1 score at threshold 0.28 is 0.6789667896678966\n",
      "F1 score at threshold 0.29 is 0.6754221388367729\n",
      "F1 score at threshold 0.3 is 0.683111954459203\n",
      "F1 score at threshold 0.31 is 0.6807692307692308\n",
      "F1 score at threshold 0.32 is 0.683495145631068\n",
      "F1 score at threshold 0.33 is 0.6771037181996087\n",
      "F1 score at threshold 0.34 is 0.6746031746031745\n",
      "F1 score at threshold 0.35 is 0.6733067729083665\n",
      "F1 score at threshold 0.36 is 0.672\n",
      "F1 score at threshold 0.37 is 0.6680161943319837\n",
      "F1 score at threshold 0.38 is 0.6735966735966736\n",
      "F1 score at threshold 0.39 is 0.6751054852320675\n",
      "F1 score at threshold 0.4 is 0.6809421841541756\n",
      "F1 score at threshold 0.41 is 0.6840958605664488\n",
      "F1 score at threshold 0.42 is 0.6754966887417219\n",
      "F1 score at threshold 0.43 is 0.6756756756756757\n",
      "F1 score at threshold 0.44 is 0.677345537757437\n",
      "F1 score at threshold 0.45 is 0.6666666666666667\n",
      "F1 score at threshold 0.46 is 0.6474820143884892\n",
      "F1 score at threshold 0.47 is 0.6403940886699506\n",
      "F1 score at threshold 0.48 is 0.6334164588528678\n",
      "F1 score at threshold 0.49 is 0.6205128205128205\n",
      "F1 score at threshold 0.5 is 0.612137203166227\n",
      "Best threshold:  0.41\n",
      "Optimal F1: 0.6840958605664488 at threshold: 0.41\n"
     ]
    }
   ],
   "source": [
    "f1, threshold = f1_smart(val_y, pred_val_y)\n",
    "print('Optimal F1: {} at threshold: {}'.format(f1, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the F1 score of tweet classification:0.6414662084765177\n"
     ]
    }
   ],
   "source": [
    "pred_test_y = (pred_test_y >threshold).astype(int)\n",
    "res = metrics.f1_score(test_y, pred_test_y)\n",
    "print('the F1 score of tweet classification:{}'.format(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
